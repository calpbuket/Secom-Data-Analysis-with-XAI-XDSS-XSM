"""
================================================================================
SECOM VERÄ° SETÄ° - KAPSAMLI KEÅÄ°FSEL VERÄ° ANALÄ°ZÄ° (EDA)
YarÄ± Ä°letken Ãœretim HatasÄ± Tahmini iÃ§in Akademik DÃ¼zeyde Analiz
================================================================================

Bu kod, SECOM (Semiconductor Manufacturing) veri seti Ã¼zerinde kapsamlÄ± bir
Exploratory Data Analysis (EDA) gerÃ§ekleÅŸtirmektedir.

BÃ¶lÃ¼mler:
1. Veri KÃ¼mesi TanÄ±tÄ±mÄ±
2. Eksik Veri Analizi
3. TanÄ±mlayÄ±cÄ± Ä°statistikler
4. Hedef DeÄŸiÅŸken Analizi (SÄ±nÄ±f DengesizliÄŸi)
5. Korelasyon ve Ä°liÅŸki Analizi
6. AykÄ±rÄ± DeÄŸer (Outlier) Analizi
7. Hedef DeÄŸiÅŸken ile SensÃ¶r Ä°liÅŸkisi
8. SonuÃ§ ve Ã–zet

Gerekli KÃ¼tÃ¼phaneler:
    pip install pandas numpy matplotlib seaborn scipy scikit-learn

Yazar: EDA Analiz Scripti
Tarih: 2024
================================================================================
"""

# =============================================================================
# KÃœTÃœPHANELER
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import skew, kurtosis
import warnings

# UyarÄ±larÄ± kapat
warnings.filterwarnings('ignore')

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
sns.set_style("whitegrid")
plt.rcParams['axes.facecolor'] = '#f8f9fa'

# =============================================================================
# VERÄ° YÃœKLEME
# =============================================================================
# Veri dosyasÄ±nÄ±n yolunu kendi sisteminize gÃ¶re gÃ¼ncelleyin
DATA_PATH = 'Downloads/Buket/uci-secom.csv'  
OUTPUT_DIR = './eda_outputs/'  

# Ã‡Ä±ktÄ± klasÃ¶rÃ¼nÃ¼ oluÅŸtur
import os
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# Veri yÃ¼kleme
print("Veri yÃ¼kleniyor...")
df = pd.read_csv(DATA_PATH)

# Temel deÄŸiÅŸkenler
target_col = 'Pass/Fail'
feature_cols = [col for col in df.columns if col not in ['Time', 'Pass/Fail']]

print(f"âœ“ Veri yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")


# =============================================================================
# BÃ–LÃœM 1: VERÄ° KÃœMESÄ° TANITIMI
# =============================================================================
def bolum1_veri_tanitimi(df, target_col, feature_cols):
    """Veri kÃ¼mesinin temel Ã¶zelliklerini analiz eder"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 1: VERÄ° KÃœMESÄ° TANITIMI")
    print("="*80)
    
    # Temel bilgiler
    print(f"\nğŸ“Š Veri Seti Genel Bilgileri:")
    print(f"   â€¢ SatÄ±r sayÄ±sÄ± (GÃ¶zlem): {df.shape[0]:,}")
    print(f"   â€¢ SÃ¼tun sayÄ±sÄ± (DeÄŸiÅŸken): {df.shape[1]:,}")
    print(f"   â€¢ SensÃ¶r sayÄ±sÄ±: {len(feature_cols)}")
    
    # Hedef deÄŸiÅŸken analizi
    print(f"\nğŸ¯ Hedef DeÄŸiÅŸken: '{target_col}'")
    print(f"   â€¢ Veri tipi: {df[target_col].dtype}")
    print(f"   â€¢ Benzersiz deÄŸerler: {df[target_col].unique()}")
    print(f"   â€¢ Hedef deÄŸiÅŸken tÃ¼rÃ¼: Binary (Ä°kili SÄ±nÄ±flandÄ±rma)")
    print(f"   â€¢ -1: HatasÄ±z Ã¼retim (Pass)")
    print(f"   â€¢  1: HatalÄ± Ã¼retim (Fail)")
    
    # SÃ¼tun veri tipleri Ã¶zeti
    print("\nğŸ“‹ Veri Tiplerinin Ã–zeti:")
    dtype_counts = df.dtypes.value_counts()
    for dtype, count in dtype_counts.items():
        print(f"   â€¢ {dtype}: {count} sÃ¼tun")
    
    # DetaylÄ± veri tipleri tablosu
    print("\nğŸ“‹ SÃ¼tun Veri Tipleri (Ä°lk 30):")
    dtypes_df = pd.DataFrame({
        'SÃ¼tun AdÄ±': df.columns[:30],
        'Veri Tipi': df.dtypes[:30].values
    })
    print(dtypes_df.to_string(index=False))
    
    # Ä°lk 10 satÄ±r
    print("\nğŸ“‹ Ä°lk 10 SatÄ±r (Ä°lk 10 SÃ¼tun):")
    print(df.iloc[:10, :10].to_string())
    
    # Bellek kullanÄ±mÄ±
    memory_usage = df.memory_usage(deep=True).sum() / 1024**2
    print(f"\nğŸ’¾ Bellek KullanÄ±mÄ±: {memory_usage:.2f} MB")
    
    # Ã–zet istatistikler
    summary_stats = {
        'SatÄ±r SayÄ±sÄ±': df.shape[0],
        'SÃ¼tun SayÄ±sÄ±': df.shape[1],
        'SensÃ¶r SayÄ±sÄ±': len(feature_cols),
        'Bellek (MB)': round(memory_usage, 2)
    }
    
    return summary_stats


# =============================================================================
# BÃ–LÃœM 2: EKSÄ°K VERÄ° ANALÄ°ZÄ°
# =============================================================================
def bolum2_eksik_veri_analizi(df, target_col, feature_cols, output_dir):
    """Eksik veri analizi yapar ve gÃ¶rselleÅŸtirir"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 2: EKSÄ°K VERÄ° ANALÄ°ZÄ°")
    print("="*80)
    
    # Eksik deÄŸer hesaplama
    missing_counts = df[feature_cols].isnull().sum()
    missing_percent = (df[feature_cols].isnull().sum() / len(df)) * 100
    
    missing_df = pd.DataFrame({
        'SÃ¼tun': feature_cols,
        'Eksik SayÄ±sÄ±': missing_counts.values,
        'Eksik OranÄ± (%)': missing_percent.values
    }).sort_values('Eksik OranÄ± (%)', ascending=False)
    
    # Genel eksik veri istatistikleri
    total_cells = df[feature_cols].size
    total_missing = df[feature_cols].isnull().sum().sum()
    overall_missing_pct = (total_missing / total_cells) * 100
    
    print(f"\nğŸ“Š Genel Eksik Veri Ä°statistikleri:")
    print(f"   â€¢ Toplam hÃ¼cre sayÄ±sÄ±: {total_cells:,}")
    print(f"   â€¢ Toplam eksik deÄŸer: {total_missing:,}")
    print(f"   â€¢ Genel eksik oran: %{overall_missing_pct:.2f}")
    
    # Eksik deÄŸer iÃ§eren sÃ¼tun sayÄ±larÄ±
    cols_with_missing = (missing_counts > 0).sum()
    cols_no_missing = len(feature_cols) - cols_with_missing
    print(f"\n   â€¢ Eksik deÄŸer iÃ§eren sÃ¼tun: {cols_with_missing}")
    print(f"   â€¢ Eksik deÄŸer iÃ§ermeyen sÃ¼tun: {cols_no_missing}")
    
    # En Ã§ok eksik iÃ§eren ilk 20 sÃ¼tun
    print("\nğŸ“‹ En Ã‡ok Eksik DeÄŸer Ä°Ã§eren Ä°lk 20 SÃ¼tun:")
    top20_missing = missing_df[missing_df['Eksik OranÄ± (%)'] > 0].head(20)
    print(top20_missing.to_string(index=False))
    
    # Eksik deÄŸer kategorileri
    high_missing = missing_df[missing_df['Eksik OranÄ± (%)'] > 50]
    medium_missing = missing_df[(missing_df['Eksik OranÄ± (%)'] > 20) & (missing_df['Eksik OranÄ± (%)'] <= 50)]
    low_missing = missing_df[(missing_df['Eksik OranÄ± (%)'] > 0) & (missing_df['Eksik OranÄ± (%)'] <= 20)]
    
    print(f"\nğŸ“Š Eksik Veri Kategorileri:")
    print(f"   â€¢ YÃ¼ksek eksiklik (>50%): {len(high_missing)} sÃ¼tun")
    print(f"   â€¢ Orta eksiklik (20-50%): {len(medium_missing)} sÃ¼tun")
    print(f"   â€¢ DÃ¼ÅŸÃ¼k eksiklik (0-20%): {len(low_missing)} sÃ¼tun")
    
    # Hedef deÄŸiÅŸken ile eksik veri iliÅŸkisi
    print("\nğŸ”— Eksik Veri ve Hedef DeÄŸiÅŸken Ä°liÅŸkisi:")
    df_temp = df.copy()
    df_temp['missing_count'] = df[feature_cols].isnull().sum(axis=1)
    
    # SÄ±nÄ±flara gÃ¶re eksik veri ortalamasÄ±
    missing_by_class = df_temp.groupby(target_col)['missing_count'].agg(['mean', 'std', 'min', 'max'])
    print("\n   SÄ±nÄ±f BazÄ±nda Eksik DeÄŸer Ä°statistikleri:")
    print(f"   Pass (-1): Ortalama={missing_by_class.loc[-1, 'mean']:.2f}, Std={missing_by_class.loc[-1, 'std']:.2f}")
    print(f"   Fail (1):  Ortalama={missing_by_class.loc[1, 'mean']:.2f}, Std={missing_by_class.loc[1, 'std']:.2f}")
    
    # T-testi
    pass_missing = df_temp[df_temp[target_col] == -1]['missing_count']
    fail_missing = df_temp[df_temp[target_col] == 1]['missing_count']
    t_stat, p_value = stats.ttest_ind(pass_missing, fail_missing)
    print(f"\n   T-Test Sonucu: t={t_stat:.4f}, p-value={p_value:.4f}")
    if p_value < 0.05:
        print("   âš ï¸ Ä°statistiksel olarak anlamlÄ± fark VAR (p < 0.05)")
    else:
        print("   âœ“ Ä°statistiksel olarak anlamlÄ± fark YOK (p >= 0.05)")
    
    # MCAR/MAR/MNAR analizi
    print("\n" + "-"*60)
    print("ğŸ“Š Eksik Veri MekanizmasÄ± Analizi (MCAR/MAR/MNAR)")
    print("-"*60)
    
    missing_pattern = df[feature_cols].isnull().sum(axis=1)
    print(f"\n   SatÄ±r bazÄ±nda eksik deÄŸer aralÄ±ÄŸÄ±: {missing_pattern.min()} - {missing_pattern.max()}")
    print(f"   SatÄ±r bazÄ±nda ortalama eksik: {missing_pattern.mean():.2f}")
    
    # Korelasyon analizi
    sample_cols_with_missing = missing_df[missing_df['Eksik OranÄ± (%)'] > 5]['SÃ¼tun'].head(10).tolist()
    if len(sample_cols_with_missing) > 1:
        missing_indicator = df[sample_cols_with_missing].isnull().astype(int)
        missing_corr = missing_indicator.corr().mean().mean()
        print(f"\n   Eksik deÄŸer gÃ¶stergeleri arasÄ± ortalama korelasyon: {missing_corr:.4f}")
    
    print("""
ğŸ”¬ Bilimsel Yorum - Eksik Veri YapÄ±sÄ±:

   1. MCAR (Missing Completely At Random) DEÄÄ°L:
      â€¢ BazÄ± sÃ¼tunlarda %40+'Ä±n Ã¼zerinde eksiklik bulunmasÄ±
      â€¢ Eksik deÄŸerlerin belirli sÃ¼tunlarda yoÄŸunlaÅŸmasÄ±
      MCAR varsayÄ±mÄ±nÄ± desteklememektedir.
   
   2. MAR (Missing At Random) olasÄ±lÄ±ÄŸÄ± YÃœKSEK:
      â€¢ Eksik deÄŸerlerin gÃ¶zlemlenen diÄŸer deÄŸiÅŸkenlerle iliÅŸkili olmasÄ±
      â€¢ SensÃ¶r arÄ±zalarÄ± veya Ã¶lÃ§Ã¼m koÅŸullarÄ±na baÄŸlÄ± eksiklik
      MAR mekanizmasÄ±nÄ± desteklemektedir.
   
   3. MNAR (Missing Not At Random) Ä°HTÄ°MALÄ°:
      â€¢ BazÄ± sensÃ¶rlerin limit deÄŸerlerinde kayÄ±t yapamamasÄ±
      â€¢ Ãœretim hatasÄ± durumunda sensÃ¶r Ã§alÄ±ÅŸmamasÄ±
      MNAR olasÄ±lÄ±ÄŸÄ±nÄ± dÃ¼ÅŸÃ¼ndÃ¼rmektedir.
   
   â¤ SonuÃ§: Bu veri seti muhtemelen MAR veya karma bir mekanizma gÃ¶stermektedir.
""")
    
    print("""
ğŸ“‹ Ã–nerilen Eksik Veri Stratejileri:

   1. SILME (Deletion):
      âœ“ %50+'dan fazla eksik iÃ§eren sÃ¼tunlar silinebilir
      âœ— SatÄ±r silme Ã¶nerilmez - veri kaybÄ± Ã§ok yÃ¼ksek olur
      
   2. MEAN/MEDIAN IMPUTATION:
      âœ“ Basit ve hÄ±zlÄ± uygulama
      âœ— VaryansÄ± kÃ¼Ã§Ã¼ltÃ¼r, korelasyonlarÄ± bozar
      
   3. KNN IMPUTATION:
      âœ“ GÃ¶zlemler arasÄ± benzerliÄŸi kullanÄ±r
      âœ“ Multivariate yapÄ±yÄ± korur
      âœ— YÃ¼ksek boyutlu verilerde hesaplama maliyeti
      
   4. ITERATIVE IMPUTER (MICE):
      âœ“ MAR varsayÄ±mÄ± altÄ±nda en iyi performans
      âœ“ DeÄŸiÅŸkenler arasÄ± iliÅŸkileri modelleyerek impute eder
      âœ— Hesaplama sÃ¼resi uzun
      
   â¤ Ã–NERÄ°: %40+ eksik sÃ¼tunlarÄ± silmek, kalan iÃ§in IterativeImputer
""")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Eksik deÄŸer daÄŸÄ±lÄ±mÄ± histogram
    ax1 = axes[0, 0]
    missing_pct_nonzero = missing_df[missing_df['Eksik OranÄ± (%)'] > 0]['Eksik OranÄ± (%)']
    ax1.hist(missing_pct_nonzero, bins=50, color='#e74c3c', edgecolor='black', alpha=0.7)
    ax1.set_xlabel('Eksik DeÄŸer OranÄ± (%)')
    ax1.set_ylabel('SÃ¼tun SayÄ±sÄ±')
    ax1.set_title('Eksik DeÄŸer OranlarÄ±nÄ±n DaÄŸÄ±lÄ±mÄ±')
    ax1.axvline(x=20, color='orange', linestyle='--', label='%20 EÅŸiÄŸi')
    ax1.axvline(x=50, color='red', linestyle='--', label='%50 EÅŸiÄŸi')
    ax1.legend()
    
    # 2. En Ã§ok eksik iÃ§eren 20 sÃ¼tun
    ax2 = axes[0, 1]
    top20 = missing_df.head(20)
    ax2.barh(range(len(top20)), top20['Eksik OranÄ± (%)'], color='#3498db', edgecolor='black')
    ax2.set_yticks(range(len(top20)))
    ax2.set_yticklabels(top20['SÃ¼tun'])
    ax2.set_xlabel('Eksik DeÄŸer OranÄ± (%)')
    ax2.set_title('En Ã‡ok Eksik DeÄŸer Ä°Ã§eren 20 SÃ¼tun')
    ax2.invert_yaxis()
    
    # 3. SÄ±nÄ±f bazÄ±nda eksik veri boxplot
    ax3 = axes[1, 0]
    data_boxplot = [pass_missing, fail_missing]
    bp = ax3.boxplot(data_boxplot, patch_artist=True, labels=['Pass (-1)', 'Fail (1)'])
    colors = ['#2ecc71', '#e74c3c']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    ax3.set_ylabel('SatÄ±r BaÅŸÄ±na Eksik DeÄŸer SayÄ±sÄ±')
    ax3.set_title('Hedef DeÄŸiÅŸkene GÃ¶re Eksik Veri DaÄŸÄ±lÄ±mÄ±')
    
    # 4. SatÄ±r baÅŸÄ±na eksik deÄŸer daÄŸÄ±lÄ±mÄ±
    ax4 = axes[1, 1]
    ax4.hist(df_temp['missing_count'], bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)
    ax4.set_xlabel('SatÄ±r BaÅŸÄ±na Eksik DeÄŸer SayÄ±sÄ±')
    ax4.set_ylabel('GÃ¶zlem SayÄ±sÄ±')
    ax4.set_title('GÃ¶zlemlerdeki Eksik DeÄŸer DaÄŸÄ±lÄ±mÄ±')
    ax4.axvline(x=df_temp['missing_count'].mean(), color='red', linestyle='--', 
                label=f'Ortalama: {df_temp["missing_count"].mean():.1f}')
    ax4.legend()
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig1_eksik_veri_analizi.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig1_eksik_veri_analizi.png")
    
    # Ã–zet istatistikler
    missing_stats = {
        'Toplam Eksik': total_missing,
        'Genel Eksik OranÄ±': overall_missing_pct,
        'Eksik Ä°Ã§eren SÃ¼tun': cols_with_missing,
        'YÃ¼ksek Eksik SÃ¼tun (>50%)': len(high_missing),
        'T-test p-value': p_value
    }
    
    return missing_stats, missing_df


# =============================================================================
# BÃ–LÃœM 3: TANIMLAYICI Ä°STATÄ°STÄ°KLER
# =============================================================================
def bolum3_tanimlayici_istatistikler(df, feature_cols, output_dir):
    """Her sensÃ¶r iÃ§in tanÄ±mlayÄ±cÄ± istatistikleri hesaplar"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 3: TANIMLAYICI Ä°STATÄ°STÄ°KLER")
    print("="*80)
    
    # Temel istatistikler hesaplama
    stats_list = []
    for col in feature_cols:
        col_data = df[col].dropna()
        if len(col_data) > 0:
            stats_list.append({
                'SÃ¼tun': col,
                'N': len(col_data),
                'Ortalama': col_data.mean(),
                'Medyan': col_data.median(),
                'Std': col_data.std(),
                'Min': col_data.min(),
                'Max': col_data.max(),
                'Skewness': skew(col_data) if col_data.std() > 0 else np.nan,
                'Kurtosis': kurtosis(col_data) if col_data.std() > 0 else np.nan
            })
    
    stats_df = pd.DataFrame(stats_list)
    
    print("\nğŸ“Š TanÄ±mlayÄ±cÄ± Ä°statistikler Ã–zeti (Ä°lk 20 SensÃ¶r):")
    display_cols = ['SÃ¼tun', 'N', 'Ortalama', 'Medyan', 'Std', 'Min', 'Max', 'Skewness', 'Kurtosis']
    print(stats_df[display_cols].head(20).to_string(index=False))
    
    # Skewness ve Kurtosis analizi
    valid_skew = stats_df['Skewness'].dropna()
    valid_kurt = stats_df['Kurtosis'].dropna()
    
    normal_dist = len(valid_skew[abs(valid_skew) < 0.5])
    mild_skew = len(valid_skew[(abs(valid_skew) >= 0.5) & (abs(valid_skew) < 1)])
    high_skew = len(valid_skew[abs(valid_skew) >= 1])
    
    print(f"\nğŸ“ˆ DaÄŸÄ±lÄ±m Ã–zellikleri Analizi:")
    print(f"   â€¢ Normal daÄŸÄ±lÄ±m gÃ¶steren sÃ¼tunlar (|skew| < 0.5): {normal_dist}")
    print(f"   â€¢ Hafif Ã§arpÄ±k daÄŸÄ±lÄ±mlar (0.5 <= |skew| < 1): {mild_skew}")
    print(f"   â€¢ YÃ¼ksek Ã§arpÄ±k daÄŸÄ±lÄ±mlar (|skew| >= 1): {high_skew}")
    
    # En Ã§arpÄ±k daÄŸÄ±lÄ±mlar
    print("\nğŸ“‹ En YÃ¼ksek Ã‡arpÄ±klÄ±k GÃ¶steren 10 SÃ¼tun:")
    top_skew = stats_df.dropna(subset=['Skewness']).nlargest(10, 'Skewness')[['SÃ¼tun', 'Ortalama', 'Medyan', 'Skewness', 'Kurtosis']]
    print(top_skew.to_string(index=False))
    
    # Kurtosis yorumu
    leptokurtic = len(valid_kurt[valid_kurt > 3])
    mesokurtic = len(valid_kurt[(valid_kurt >= -3) & (valid_kurt <= 3)])
    platykurtic = len(valid_kurt[valid_kurt < -3])
    
    print(f"\nğŸ“Š Kurtosis (BasÄ±klÄ±k) Analizi:")
    print(f"   â€¢ Leptokurtik (sivri, >3): {leptokurtic} sÃ¼tun")
    print(f"   â€¢ Mesokurtik (normal, -3 ile 3): {mesokurtic} sÃ¼tun")
    print(f"   â€¢ Platikurtik (basÄ±k, <-3): {platykurtic} sÃ¼tun")
    
    # AykÄ±rÄ± deÄŸer potansiyeli
    print("\nğŸ” AykÄ±rÄ± DeÄŸer Potansiyeli YÃ¼ksek SÃ¼tunlar (Kurtosis > 10):")
    outlier_potential = stats_df[stats_df['Kurtosis'] > 10][['SÃ¼tun', 'Min', 'Max', 'Skewness', 'Kurtosis']].head(10)
    print(outlier_potential.to_string(index=False))
    
    print("""
ğŸ“ YORUM: TanÄ±mlayÄ±cÄ± Ä°statistikler
   
   â€¢ SensÃ¶r verilerinin bÃ¼yÃ¼k Ã§oÄŸunluÄŸu normal daÄŸÄ±lÄ±m gÃ¶stermemektedir.
   â€¢ YÃ¼ksek skewness deÄŸerleri (>5) logaritmik dÃ¶nÃ¼ÅŸÃ¼m ihtiyacÄ±nÄ± gÃ¶sterir.
   â€¢ YÃ¼ksek kurtosis deÄŸerleri (>10) uÃ§ deÄŸerlerin varlÄ±ÄŸÄ±na iÅŸaret eder.
   â€¢ Ortalama-medyan farklarÄ± daÄŸÄ±lÄ±mlarÄ±n simetrik olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir.
   â€¢ Ã–lÃ§eklendirme (StandardScaler/RobustScaler) kesinlikle gereklidir.
""")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Skewness daÄŸÄ±lÄ±mÄ±
    ax1 = axes[0, 0]
    skew_clipped = valid_skew.clip(-10, 10)
    ax1.hist(skew_clipped, bins=50, color='#3498db', edgecolor='black', alpha=0.7)
    ax1.axvline(x=0, color='red', linestyle='--', label='Normal (skew=0)')
    ax1.axvline(x=-0.5, color='orange', linestyle=':', label='Â±0.5 sÄ±nÄ±rÄ±')
    ax1.axvline(x=0.5, color='orange', linestyle=':')
    ax1.set_xlabel('Skewness (Ã‡arpÄ±klÄ±k)')
    ax1.set_ylabel('SÃ¼tun SayÄ±sÄ±')
    ax1.set_title('SensÃ¶rlerin Ã‡arpÄ±klÄ±k DaÄŸÄ±lÄ±mÄ±')
    ax1.legend()
    
    # 2. Kurtosis daÄŸÄ±lÄ±mÄ±
    ax2 = axes[0, 1]
    kurt_clipped = valid_kurt.clip(-5, 50)
    ax2.hist(kurt_clipped, bins=50, color='#e74c3c', edgecolor='black', alpha=0.7)
    ax2.axvline(x=0, color='red', linestyle='--', label='Normal (kurtosis=0)')
    ax2.set_xlabel('Kurtosis (BasÄ±klÄ±k)')
    ax2.set_ylabel('SÃ¼tun SayÄ±sÄ±')
    ax2.set_title('SensÃ¶rlerin BasÄ±klÄ±k DaÄŸÄ±lÄ±mÄ±')
    ax2.legend()
    
    # 3. Ã–rnek normal daÄŸÄ±lÄ±m gÃ¶steren sensÃ¶r
    normal_sensors = stats_df[(abs(stats_df['Skewness']) < 0.5) & (abs(stats_df['Kurtosis']) < 3)]['SÃ¼tun'].head(3).tolist()
    ax3 = axes[1, 0]
    if normal_sensors:
        for sensor in normal_sensors[:3]:
            data = df[sensor].dropna()
            ax3.hist(data, bins=30, alpha=0.5, label=f'SensÃ¶r {sensor}', edgecolor='black')
        ax3.set_xlabel('DeÄŸer')
        ax3.set_ylabel('Frekans')
        ax3.set_title('Normal DaÄŸÄ±lÄ±ma YakÄ±n SensÃ¶rler')
        ax3.legend()
    
    # 4. Ã–rnek Ã§arpÄ±k daÄŸÄ±lÄ±m gÃ¶steren sensÃ¶r
    skewed_sensors = stats_df[stats_df['Skewness'] > 5]['SÃ¼tun'].head(3).tolist()
    ax4 = axes[1, 1]
    if skewed_sensors:
        for sensor in skewed_sensors[:3]:
            data = df[sensor].dropna()
            ax4.hist(data, bins=30, alpha=0.5, label=f'SensÃ¶r {sensor}', edgecolor='black')
        ax4.set_xlabel('DeÄŸer')
        ax4.set_ylabel('Frekans')
        ax4.set_title('YÃ¼ksek Ã‡arpÄ±klÄ±k GÃ¶steren SensÃ¶rler')
        ax4.legend()
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig2_tanimlayici_istatistikler.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig2_tanimlayici_istatistikler.png")
    
    return stats_df


# =============================================================================
# BÃ–LÃœM 4: HEDEF DEÄÄ°ÅKEN ANALÄ°ZÄ° (SINIF DENGESÄ°ZLÄ°ÄÄ°)
# =============================================================================
def bolum4_sinif_dengesizligi(df, target_col, output_dir):
    """Hedef deÄŸiÅŸken ve sÄ±nÄ±f dengesizliÄŸi analizi"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 4: HEDEF DEÄÄ°ÅKEN ANALÄ°ZÄ° (SINIF DENGESÄ°ZLÄ°ÄÄ°)")
    print("="*80)
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
    class_counts = df[target_col].value_counts()
    class_percent = df[target_col].value_counts(normalize=True) * 100
    
    print(f"\nğŸ“Š SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
    print(f"   â€¢ Pass (-1): {class_counts[-1]:,} gÃ¶zlem (%{class_percent[-1]:.2f})")
    print(f"   â€¢ Fail (1):  {class_counts[1]:,} gÃ¶zlem (%{class_percent[1]:.2f})")
    
    # Dengesizlik oranÄ±
    imbalance_ratio = class_counts[-1] / class_counts[1]
    print(f"\nğŸ“Š Dengesizlik OranÄ±: {imbalance_ratio:.2f}:1 (Pass:Fail)")
    
    print("""
âš ï¸ SINIF DENGESÄ°ZLÄ°ÄÄ° SORUNU:
   
   Bu veri setinde ciddi bir sÄ±nÄ±f dengesizliÄŸi bulunmaktadÄ±r:
   â€¢ HatalÄ± Ã¼rÃ¼nler (Fail) toplam verinin sadece ~%6'sÄ±nÄ± oluÅŸturur
   â€¢ Bu tÃ¼r dengesizlik, modelin Ã§oÄŸunluk sÄ±nÄ±fÄ±na (Pass) aÅŸÄ±rÄ± Ã¶ÄŸrenmesine neden olur
""")
    
    print("""
ğŸ“Š METRÄ°K SEÃ‡Ä°MÄ° AÃ‡IKLAMASI:

   1. ACCURACY (DoÄŸruluk) NEDEN YETERSÄ°Z?
      â€¢ TÃ¼m gÃ¶zlemleri "Pass" tahmin eden bir model %93+ accuracy elde eder
      â€¢ Bu yanÄ±ltÄ±cÄ±dÄ±r Ã§Ã¼nkÃ¼ hiÃ§bir hatalÄ± Ã¼rÃ¼n tespit edilemez
      â€¢ Ãœretim hattÄ±nda kaÃ§Ä±rÄ±lan her hatalÄ± Ã¼rÃ¼n maliyetli sonuÃ§lar doÄŸurur
   
   2. PRECISION (Kesinlik):
      â€¢ HatalÄ± tahmin edilenlerin ne kadarÄ± gerÃ§ekten hatalÄ±?
      â€¢ YanlÄ±ÅŸ alarm (false positive) maliyetini Ã¶lÃ§er
   
   3. RECALL (DuyarlÄ±lÄ±k/Sensitivity):
      â€¢ GerÃ§ek hatalÄ± Ã¼rÃ¼nlerin ne kadarÄ± yakalandÄ±?
      â€¢ KaÃ§Ä±rÄ±lan hata (false negative) maliyetini Ã¶lÃ§er
   
   4. F1-SCORE:
      â€¢ Precision ve Recall'un harmonik ortalamasÄ±
      â€¢ Dengesiz sÄ±nÄ±flarda tek bir metrik olarak idealdir
   
   5. ROC-AUC / PR-AUC:
      â€¢ SÄ±nÄ±f oranlarÄ±ndan baÄŸÄ±msÄ±z performans Ã¶lÃ§Ã¼mÃ¼
      â€¢ Dengesiz verilerde PR-AUC daha bilgilendirici
""")
    
    print("""
ğŸ”§ Ã–NERÄ°LEN DENGESÄ°ZLÄ°K Ã‡Ã–ZÃœM STRATEJÄ°LERÄ°:

   1. RESAMPLING TEKNÄ°KLERÄ°:
      â€¢ SMOTE (Synthetic Minority Over-sampling)
      â€¢ ADASYN (Adaptive Synthetic Sampling)
      â€¢ Random Undersampling
      
   2. SINIF AÄIRLIKLANDIRMA:
      â€¢ class_weight='balanced' parametresi
      
   3. ENSEMBLE YÃ–NTEMLER:
      â€¢ BalancedRandomForest
      â€¢ EasyEnsemble
      
   4. THRESHOLD OPTÄ°MÄ°ZASYONU:
      â€¢ Precision-Recall eÄŸrisi ile optimal threshold
""")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # 1. Bar plot
    ax1 = axes[0]
    colors = ['#2ecc71', '#e74c3c']
    bars = ax1.bar(['Pass (-1)', 'Fail (1)'], [class_counts[-1], class_counts[1]], 
                   color=colors, edgecolor='black')
    ax1.set_ylabel('GÃ¶zlem SayÄ±sÄ±')
    ax1.set_title('Hedef DeÄŸiÅŸken SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')
    for bar, count, pct in zip(bars, [class_counts[-1], class_counts[1]], 
                               [class_percent[-1], class_percent[1]]):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, 
                 f'{count:,}\n(%{pct:.1f})', ha='center', va='bottom', 
                 fontsize=12, fontweight='bold')
    
    # 2. Pie chart
    ax2 = axes[1]
    explode = (0, 0.1)
    ax2.pie([class_counts[-1], class_counts[1]], labels=['Pass (-1)', 'Fail (1)'], 
            autopct='%1.1f%%', colors=colors, explode=explode, shadow=True, startangle=90)
    ax2.set_title('SÄ±nÄ±f OranlarÄ±')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig3_sinif_dengesizligi.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig3_sinif_dengesizligi.png")
    
    class_stats = {
        'Pass Count': class_counts[-1],
        'Fail Count': class_counts[1],
        'Pass Percent': class_percent[-1],
        'Fail Percent': class_percent[1],
        'Imbalance Ratio': imbalance_ratio
    }
    
    return class_stats


# =============================================================================
# BÃ–LÃœM 5: KORELASYON VE Ä°LÄ°ÅKÄ° ANALÄ°ZÄ°
# =============================================================================
def bolum5_korelasyon_analizi(df, target_col, feature_cols, output_dir):
    """Korelasyon matrisi ve multicollinearity analizi"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 5: KORELASYON VE Ä°LÄ°ÅKÄ° ANALÄ°ZÄ°")
    print("="*80)
    
    # Eksik deÄŸerleri geÃ§ici olarak doldur
    df_temp = df[feature_cols].fillna(df[feature_cols].median())
    
    # Korelasyon matrisi hesaplama
    print("\nğŸ“Š Korelasyon Matrisi HesaplanÄ±yor...")
    corr_matrix = df_temp.corr()
    
    # En yÃ¼ksek korelasyonlar
    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    corr_pairs = []
    for col in upper_tri.columns:
        for idx in upper_tri.index:
            if pd.notna(upper_tri.loc[idx, col]):
                corr_pairs.append({
                    'Ã–zellik 1': idx,
                    'Ã–zellik 2': col,
                    'Korelasyon': upper_tri.loc[idx, col]
                })
    
    corr_pairs_df = pd.DataFrame(corr_pairs)
    corr_pairs_df['Abs_Corr'] = abs(corr_pairs_df['Korelasyon'])
    corr_pairs_df = corr_pairs_df.sort_values('Abs_Corr', ascending=False)
    
    print("\nğŸ“‹ En YÃ¼ksek Korelasyona Sahip Ä°lk 20 Ã–zellik Ã‡ifti:")
    top20_corr = corr_pairs_df.head(20)[['Ã–zellik 1', 'Ã–zellik 2', 'Korelasyon']]
    print(top20_corr.to_string(index=False))
    
    # YÃ¼ksek korelasyon istatistikleri
    very_high_corr = len(corr_pairs_df[corr_pairs_df['Abs_Corr'] > 0.95])
    high_corr = len(corr_pairs_df[(corr_pairs_df['Abs_Corr'] > 0.8) & (corr_pairs_df['Abs_Corr'] <= 0.95)])
    moderate_corr = len(corr_pairs_df[(corr_pairs_df['Abs_Corr'] > 0.5) & (corr_pairs_df['Abs_Corr'] <= 0.8)])
    
    print(f"\nğŸ“Š Korelasyon DÃ¼zeyleri:")
    print(f"   â€¢ Ã‡ok yÃ¼ksek korelasyon (|r| > 0.95): {very_high_corr:,} Ã§ift")
    print(f"   â€¢ YÃ¼ksek korelasyon (0.8 < |r| <= 0.95): {high_corr:,} Ã§ift")
    print(f"   â€¢ Orta korelasyon (0.5 < |r| <= 0.8): {moderate_corr:,} Ã§ift")
    
    # Hedef deÄŸiÅŸken ile korelasyonlar
    df_temp['target'] = df[target_col]
    target_corr = df_temp.corr()['target'].drop('target').sort_values(key=abs, ascending=False)
    
    print("\nğŸ“‹ Hedef DeÄŸiÅŸken ile En YÃ¼ksek Korelasyona Sahip 20 Ã–zellik:")
    target_corr_df = pd.DataFrame({
        'Ã–zellik': target_corr.head(20).index,
        'Korelasyon': target_corr.head(20).values
    })
    print(target_corr_df.to_string(index=False))
    
    print("""
âš ï¸ MULTICOLLINEARITY (Ã‡OKLU DOÄRUSALLLIK) RÄ°SKÄ°:

   Veri setinde ciddi multicollinearity problemi bulunmaktadÄ±r:
   
   SORUNLAR:
   â€¢ YÃ¼zlerce Ã¶zellik Ã§ifti arasÄ±nda r > 0.95 korelasyon
   â€¢ Bu, Ã¶zelliklerin birbirinin kopyasÄ± veya tÃ¼revi olduÄŸunu gÃ¶sterir
   â€¢ Regresyon modellerinde katsayÄ± tahminlerini dengesizleÅŸtirir
   
   NEDENLER:
   â€¢ AynÄ± sensÃ¶rÃ¼n farklÄ± zaman dilimlerindeki Ã¶lÃ§Ã¼mleri
   â€¢ TÃ¼retilmiÅŸ Ã¶zellikler (Ã¶r: ortalama, toplam)
   â€¢ Fiziksel olarak iliÅŸkili sensÃ¶rler
""")
    
    print("""
ğŸ”§ PCA VE FEATURE SELECTION Ä°HTÄ°YACI:

   âœ“ PCA (Principal Component Analysis) KESÄ°NLÄ°KLE Ã–NERÄ°LÄ°R:
   â€¢ 590 Ã¶zellik Ã§ok yÃ¼ksek boyutluluk
   â€¢ Ã–nerilen: %95 varyans aÃ§Ä±klayan bileÅŸenler (~50-100)
   
   âœ“ FEATURE SELECTION STRATEJÄ°LERÄ°:
   â€¢ Variance Threshold: DÃ¼ÅŸÃ¼k varyanslÄ± sÃ¼tunlarÄ± kaldÄ±r
   â€¢ Korelasyon BazlÄ± Eleme: r > 0.95 olan Ã§iftlerden birini kaldÄ±r
   â€¢ Tree-based Feature Importance
   â€¢ LASSO Regularization
""")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. Korelasyon daÄŸÄ±lÄ±mÄ±
    ax1 = axes[0]
    all_corrs = upper_tri.values.flatten()
    all_corrs = all_corrs[~np.isnan(all_corrs)]
    ax1.hist(all_corrs, bins=100, color='#9b59b6', edgecolor='black', alpha=0.7)
    ax1.axvline(x=0.8, color='red', linestyle='--', label='|r|=0.8')
    ax1.axvline(x=-0.8, color='red', linestyle='--')
    ax1.axvline(x=0.95, color='darkred', linestyle=':', label='|r|=0.95')
    ax1.axvline(x=-0.95, color='darkred', linestyle=':')
    ax1.set_xlabel('Korelasyon KatsayÄ±sÄ± (r)')
    ax1.set_ylabel('Ã–zellik Ã‡ifti SayÄ±sÄ±')
    ax1.set_title('TÃ¼m Ã–zellik Ã‡iftlerinin Korelasyon DaÄŸÄ±lÄ±mÄ±')
    ax1.legend()
    
    # 2. Hedef ile korelasyonlar
    ax2 = axes[1]
    top_target_corr = target_corr.head(15)
    colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_target_corr.values]
    ax2.barh(range(len(top_target_corr)), top_target_corr.values, color=colors, edgecolor='black')
    ax2.set_yticks(range(len(top_target_corr)))
    ax2.set_yticklabels(top_target_corr.index)
    ax2.set_xlabel('Korelasyon KatsayÄ±sÄ±')
    ax2.set_title('Hedef DeÄŸiÅŸken ile En YÃ¼ksek Korelasyonlu Ã–zellikler')
    ax2.invert_yaxis()
    ax2.axvline(x=0, color='black', linewidth=0.5)
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig4_korelasyon_analizi.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig4_korelasyon_analizi.png")
    
    # Heatmap (en Ã¶nemli 20 Ã¶zellik)
    top_features = target_corr.head(20).index.tolist()
    small_corr = df_temp[top_features].corr()
    
    fig, ax = plt.subplots(figsize=(14, 12))
    mask = np.triu(np.ones_like(small_corr, dtype=bool))
    sns.heatmap(small_corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', 
                center=0, square=True, linewidths=0.5, ax=ax, annot_kws={'size': 8})
    ax.set_title('Hedef ile En Korele 20 Ã–zelliÄŸin Korelasyon Matrisi')
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig5_korelasyon_heatmap.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ GÃ¶rsel kaydedildi: {output_dir}fig5_korelasyon_heatmap.png")
    
    corr_stats = {
        'Very High Corr (>0.95)': very_high_corr,
        'High Corr (0.8-0.95)': high_corr,
        'Moderate Corr (0.5-0.8)': moderate_corr,
        'Max Target Corr': target_corr.iloc[0],
        'Top Correlated Feature': target_corr.index[0]
    }
    
    return corr_stats, target_corr


# =============================================================================
# BÃ–LÃœM 6: AYKIRI DEÄER (OUTLIER) ANALÄ°ZÄ°
# =============================================================================
def bolum6_aykiri_deger_analizi(df, feature_cols, output_dir):
    """IQR ve Z-score ile aykÄ±rÄ± deÄŸer analizi"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 6: AYKIRI DEÄER (OUTLIER) ANALÄ°ZÄ°")
    print("="*80)
    
    # IQR yÃ¶ntemi ile aykÄ±rÄ± deÄŸer tespiti
    def count_outliers_iqr(series):
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outliers = ((series < lower) | (series > upper)).sum()
        return outliers, lower, upper
    
    # Her sÃ¼tun iÃ§in aykÄ±rÄ± deÄŸer analizi
    outlier_stats = []
    for col in feature_cols:
        col_data = df[col].dropna()
        if len(col_data) > 0 and col_data.std() > 0:
            iqr_count, lower, upper = count_outliers_iqr(col_data)
            outlier_stats.append({
                'SÃ¼tun': col,
                'GÃ¶zlem': len(col_data),
                'IQR_Outlier': iqr_count,
                'IQR_Oran(%)': (iqr_count / len(col_data)) * 100
            })
    
    outlier_df = pd.DataFrame(outlier_stats)
    outlier_df = outlier_df.sort_values('IQR_Oran(%)', ascending=False)
    
    print("\nğŸ“Š AykÄ±rÄ± DeÄŸer Ä°statistikleri (IQR YÃ¶ntemi):")
    print(f"   â€¢ Toplam analiz edilen sÃ¼tun: {len(outlier_df)}")
    print(f"   â€¢ AykÄ±rÄ± deÄŸer iÃ§eren sÃ¼tun: {(outlier_df['IQR_Outlier'] > 0).sum()}")
    
    # En Ã§ok aykÄ±rÄ± deÄŸer iÃ§eren sÃ¼tunlar
    print("\nğŸ“‹ En Ã‡ok AykÄ±rÄ± DeÄŸer Ä°Ã§eren Ä°lk 20 SÃ¼tun (IQR):")
    top20_outlier = outlier_df.head(20)[['SÃ¼tun', 'GÃ¶zlem', 'IQR_Outlier', 'IQR_Oran(%)']]
    print(top20_outlier.to_string(index=False))
    
    # Kategorizasyon
    high_outlier = len(outlier_df[outlier_df['IQR_Oran(%)'] > 10])
    medium_outlier = len(outlier_df[(outlier_df['IQR_Oran(%)'] > 5) & (outlier_df['IQR_Oran(%)'] <= 10)])
    low_outlier = len(outlier_df[(outlier_df['IQR_Oran(%)'] > 0) & (outlier_df['IQR_Oran(%)'] <= 5)])
    
    print(f"\nğŸ“Š AykÄ±rÄ± DeÄŸer Kategorileri:")
    print(f"   â€¢ YÃ¼ksek aykÄ±rÄ±lÄ±k (>%10): {high_outlier} sÃ¼tun")
    print(f"   â€¢ Orta aykÄ±rÄ±lÄ±k (%5-%10): {medium_outlier} sÃ¼tun")
    print(f"   â€¢ DÃ¼ÅŸÃ¼k aykÄ±rÄ±lÄ±k (<%5): {low_outlier} sÃ¼tun")
    
    avg_outlier = outlier_df['IQR_Oran(%)'].mean()
    max_outlier = outlier_df['IQR_Oran(%)'].max()
    
    print(f"\n   â€¢ Ortalama aykÄ±rÄ± deÄŸer oranÄ±: %{avg_outlier:.2f}")
    print(f"   â€¢ Maksimum aykÄ±rÄ± deÄŸer oranÄ±: %{max_outlier:.2f}")
    
    print("""
ğŸ”¬ AYKIRI DEÄERLERÄ°N OLASI NEDENLERÄ°:

   1. SENSÃ–R HATASI:
      â€¢ Kalibrasyon sorunlarÄ±
      â€¢ SensÃ¶r arÄ±zasÄ± veya bozulmasÄ±
      â€¢ Ä°letiÅŸim hatasÄ±
   
   2. Ã–LÃ‡ÃœM ARIZASI:
      â€¢ GeÃ§ici elektrik kesintileri
      â€¢ Ortam koÅŸullarÄ±ndaki ani deÄŸiÅŸimler
   
   3. GERÃ‡EK ÃœRETÄ°M PROBLEMÄ°:
      â€¢ Anormal Ã¼retim koÅŸullarÄ±
      â€¢ Ham madde kalite sapmalarÄ±
      â€¢ Bu deÄŸerler Ã¶nemli bilgi taÅŸÄ±yabilir!
""")
    
    print("""
âš ï¸ AYKIRI DEÄERLERÄ° SÄ°LMENÄ°N RÄ°SKLERÄ°:

   1. BÄ°LGÄ° KAYBI:
      â€¢ AykÄ±rÄ± deÄŸerler Ã¼retim hatasÄ± sinyali olabilir
      â€¢ Ã–zellikle Fail sÄ±nÄ±fÄ± iÃ§in kritik Ã¶zellikler silinebilir
   
   2. Ã–NERÄ°LEN YAKLAÅIMLAR:
      â€¢ Winsorization (%1-99 percentile)
      â€¢ RobustScaler kullanÄ±mÄ±
      â€¢ Tree-based modeller (aykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±)
      â€¢ AykÄ±rÄ± deÄŸer gÃ¶stergesi (flag) yeni Ã¶zellik olarak
""")
    
    # GÃ¶rselleÅŸtirme - Boxplotlar
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    top_outlier_cols = outlier_df.head(8)['SÃ¼tun'].tolist()
    
    for idx, ax in enumerate(axes.flatten()):
        cols_to_plot = top_outlier_cols[idx*2:(idx+1)*2]
        if cols_to_plot:
            data_to_plot = [df[col].dropna() for col in cols_to_plot]
            bp = ax.boxplot(data_to_plot, patch_artist=True, labels=cols_to_plot)
            colors = ['#3498db', '#e74c3c']
            for patch, color in zip(bp['boxes'], colors[:len(cols_to_plot)]):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)
            ax.set_title(f'SensÃ¶r {cols_to_plot[0]} ve {cols_to_plot[1]} - Boxplot')
            ax.set_ylabel('DeÄŸer')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig6_boxplots.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig6_boxplots.png")
    
    # AykÄ±rÄ± deÄŸer daÄŸÄ±lÄ±mÄ±
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.hist(outlier_df['IQR_Oran(%)'], bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)
    ax.axvline(x=5, color='orange', linestyle='--', label='%5 eÅŸiÄŸi')
    ax.axvline(x=10, color='red', linestyle='--', label='%10 eÅŸiÄŸi')
    ax.set_xlabel('AykÄ±rÄ± DeÄŸer OranÄ± (%)')
    ax.set_ylabel('SÃ¼tun SayÄ±sÄ±')
    ax.set_title('TÃ¼m SensÃ¶rlerdeki AykÄ±rÄ± DeÄŸer OranlarÄ± DaÄŸÄ±lÄ±mÄ±')
    ax.legend()
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig7_outlier_dagilim.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ GÃ¶rsel kaydedildi: {output_dir}fig7_outlier_dagilim.png")
    
    outlier_summary = {
        'Avg Outlier Rate': avg_outlier,
        'Max Outlier Rate': max_outlier,
        'High Outlier Cols (>10%)': high_outlier,
        'Medium Outlier Cols (5-10%)': medium_outlier
    }
    
    return outlier_summary, outlier_df


# =============================================================================
# BÃ–LÃœM 7: HEDEF DEÄÄ°ÅKEN Ä°LE SENSÃ–R Ä°LÄ°ÅKÄ°SÄ°
# =============================================================================
def bolum7_hedef_sensor_iliskisi(df, target_col, feature_cols, target_corr, output_dir):
    """Hedef deÄŸiÅŸken ile en iliÅŸkili sensÃ¶rlerin detaylÄ± analizi"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 7: HEDEF DEÄÄ°ÅKEN Ä°LE SENSÃ–R Ä°LÄ°ÅKÄ°SÄ°")
    print("="*80)
    
    # En Ã¶nemli 10 sensÃ¶r
    important_sensors = target_corr.head(10).index.tolist()
    
    print(f"\nğŸ“‹ Hedef DeÄŸiÅŸken ile En Ä°liÅŸkili 10 SensÃ¶r:")
    for i, sensor in enumerate(important_sensors, 1):
        corr_val = target_corr[sensor]
        print(f"   {i}. SensÃ¶r {sensor}: r = {corr_val:.4f}")
    
    # Pass ve Fail gruplarÄ± iÃ§in istatistikler
    print("\nğŸ“Š Ã–nemli SensÃ¶rlerin SÄ±nÄ±f BazÄ±nda Ä°statistikleri:")
    
    sensor_analysis = []
    for sensor in important_sensors[:5]:
        pass_data = df[df[target_col] == -1][sensor].dropna()
        fail_data = df[df[target_col] == 1][sensor].dropna()
        
        print(f"\n   SensÃ¶r {sensor}:")
        print(f"      Pass (-1): Mean={pass_data.mean():.4f}, Std={pass_data.std():.4f}")
        print(f"      Fail (1):  Mean={fail_data.mean():.4f}, Std={fail_data.std():.4f}")
        
        # T-test
        t_stat, p_val = stats.ttest_ind(pass_data, fail_data)
        print(f"      T-test: t={t_stat:.3f}, p={p_val:.4f}", end="")
        if p_val < 0.05:
            print(" âœ“ AnlamlÄ±")
        else:
            print(" âœ— AnlamsÄ±z")
        
        sensor_analysis.append({
            'Sensor': sensor,
            'Pass Mean': pass_data.mean(),
            'Fail Mean': fail_data.mean(),
            't-stat': t_stat,
            'p-value': p_val
        })
    
    print("""
ğŸ­ OPERASYONEL YORUM:

   Bu analiz sonuÃ§larÄ± Ã¼retim sÃ¼recinde ÅŸu anlamlarÄ± taÅŸÄ±maktadÄ±r:
   
   1. KRÄ°TÄ°K SENSÃ–RLER:
      â€¢ En yÃ¼ksek korelasyonlu sensÃ¶rler kalite ile doÄŸrudan iliÅŸkili
      â€¢ GerÃ§ek zamanlÄ± izleme Ã¶nceliÄŸi bu sensÃ¶rlere verilmeli
   
   2. ERKEN UYARI SÄ°STEMÄ°:
      â€¢ Pass ve Fail gruplarÄ± arasÄ±nda anlamlÄ± fark gÃ¶steren sensÃ¶rler
      â€¢ Threshold deÄŸerler belirlenerek alarm sistemi kurulabilir
   
   3. MALÄ°YET ETKÄ°SÄ°:
      â€¢ HatalÄ± Ã¼rÃ¼n tespiti erken yapÄ±labilir
      â€¢ Hurda ve yeniden iÅŸleme maliyetleri azaltÄ±labilir
""")
    
    # GÃ¶rselleÅŸtirme - Density plots
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, sensor in enumerate(important_sensors[:6]):
        ax = axes[idx]
        
        pass_data = df[df[target_col] == -1][sensor].dropna()
        fail_data = df[df[target_col] == 1][sensor].dropna()
        
        if len(pass_data) > 1:
            pass_data.plot(kind='kde', ax=ax, color='#2ecc71', label='Pass (-1)', linewidth=2)
        if len(fail_data) > 1:
            fail_data.plot(kind='kde', ax=ax, color='#e74c3c', label='Fail (1)', linewidth=2)
        
        ax.set_xlabel('SensÃ¶r DeÄŸeri')
        ax.set_ylabel('YoÄŸunluk')
        ax.set_title(f'SensÃ¶r {sensor} - SÄ±nÄ±f DaÄŸÄ±lÄ±mlarÄ±\n(r = {target_corr[sensor]:.4f})')
        ax.legend()
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig8_density_plots.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"\nâœ“ GÃ¶rsel kaydedildi: {output_dir}fig8_density_plots.png")
    
    # Violin plots
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, sensor in enumerate(important_sensors[:6]):
        ax = axes[idx]
        
        data_pass = df[df[target_col] == -1][sensor].dropna()
        data_fail = df[df[target_col] == 1][sensor].dropna()
        
        plot_data = pd.DataFrame({
            'DeÄŸer': pd.concat([data_pass, data_fail]),
            'SÄ±nÄ±f': ['Pass']*len(data_pass) + ['Fail']*len(data_fail)
        })
        
        sns.violinplot(x='SÄ±nÄ±f', y='DeÄŸer', data=plot_data, ax=ax, 
                       palette={'Pass': '#2ecc71', 'Fail': '#e74c3c'})
        ax.set_title(f'SensÃ¶r {sensor} - Violin Plot')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig9_violin_plots.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ GÃ¶rsel kaydedildi: {output_dir}fig9_violin_plots.png")
    
    # Boxplot karÅŸÄ±laÅŸtÄ±rmasÄ±
    fig, axes = plt.subplots(2, 4, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, sensor in enumerate(important_sensors[:8]):
        ax = axes[idx]
        
        data_pass = df[df[target_col] == -1][sensor].dropna()
        data_fail = df[df[target_col] == 1][sensor].dropna()
        
        bp = ax.boxplot([data_pass, data_fail], patch_artist=True, labels=['Pass', 'Fail'])
        bp['boxes'][0].set_facecolor('#2ecc71')
        bp['boxes'][1].set_facecolor('#e74c3c')
        bp['boxes'][0].set_alpha(0.7)
        bp['boxes'][1].set_alpha(0.7)
        
        ax.set_title(f'SensÃ¶r {sensor}')
        ax.set_ylabel('DeÄŸer')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}fig10_boxplot_karsilastirma.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ GÃ¶rsel kaydedildi: {output_dir}fig10_boxplot_karsilastirma.png")
    
    return pd.DataFrame(sensor_analysis)


# =============================================================================
# BÃ–LÃœM 8: SONUÃ‡ VE Ã–ZET
# =============================================================================
def bolum8_sonuc_ozet(df, feature_cols, target_col, missing_stats, class_stats, 
                      corr_stats, outlier_summary):
    """EDA sonuÃ§larÄ±nÄ±n Ã¶zeti ve model kurulum Ã¶nerileri"""
    
    print("\n" + "="*80)
    print("BÃ–LÃœM 8: SONUÃ‡ VE Ã–ZET")
    print("="*80)
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    EDA SONUÃ‡ RAPORU - SECOM VERÄ° SETÄ°                        â•‘
â•‘              YarÄ± Ä°letken Ãœretim HatasÄ± Tahmin Analizi                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    
    # 1. VERÄ° KALÄ°TESÄ°
    print("="*60)
    print("1. VERÄ° KALÄ°TESÄ° DEÄERLENDÄ°RMESÄ°")
    print("="*60)
    
    const_cols = df[feature_cols].std().fillna(0).eq(0).sum()
    
    print(f"""
   VERÄ° KALÄ°TESÄ° SKORU: ORTA-DÃœÅÃœK

   OLUMLU YÃ–NLER:
   â€¢ Toplam {len(df):,} gÃ¶zlem mevcut - yeterli Ã¶rnek bÃ¼yÃ¼klÃ¼ÄŸÃ¼
   â€¢ {len(feature_cols)} sensÃ¶r verisi - zengin Ã¶zellik uzayÄ±
   â€¢ Hedef deÄŸiÅŸken tamamen dolu
   
   SORUNLU YÃ–NLER:
   â€¢ Genel eksik veri oranÄ±: %{missing_stats['Genel Eksik OranÄ±']:.2f}
   â€¢ %50+ eksik iÃ§eren sÃ¼tun sayÄ±sÄ±: {missing_stats['YÃ¼ksek Eksik SÃ¼tun (>50%)']}
   â€¢ Sabit deÄŸerli sÃ¼tun sayÄ±sÄ±: {const_cols}
""")
    
    # 2. EKSÄ°K VERÄ° PROBLEMÄ°
    print("="*60)
    print("2. EKSÄ°K VERÄ° PROBLEMÄ° CÄ°DDÄ°YETÄ°: YÃœKSEK")
    print("="*60)
    
    print(f"""
   â€¢ Toplam eksik hÃ¼cre: {missing_stats['Toplam Eksik']:,}
   â€¢ Eksik deÄŸer iÃ§eren sÃ¼tun: {missing_stats['Eksik Ä°Ã§eren SÃ¼tun']}
   
   Ã–NERÄ°: %40+ eksik sÃ¼tunlarÄ± sil, kalan iÃ§in IterativeImputer
""")
    
    # 3. SINIF DENGESÄ°ZLÄ°ÄÄ°
    print("="*60)
    print("3. SINIF DENGESÄ°ZLÄ°ÄÄ°: KRÄ°TÄ°K")
    print("="*60)
    
    print(f"""
   â€¢ Pass:Fail oranÄ± = {class_stats['Imbalance Ratio']:.1f}:1
   â€¢ Fail oranÄ±: %{class_stats['Fail Percent']:.2f}
   
   Ã–NERÄ°: SMOTE, class_weight='balanced', F1-Score kullan
""")
    
    # 4. BOYUT AZALTMA
    print("="*60)
    print("4. BOYUT AZALTMA GEREKSÄ°NÄ°MÄ°: KRÄ°TÄ°K")
    print("="*60)
    
    print(f"""
   â€¢ |r| > 0.95 korelasyonlu Ã§ift sayÄ±sÄ±: {corr_stats['Very High Corr (>0.95)']}
   â€¢ Ã–zellik/GÃ¶zlem oranÄ±: {len(feature_cols)/len(df):.2f}
   
   Ã–NERÄ°: PCA (%95 varyans) veya korelasyon bazlÄ± eleme
""")
    
    # 5. AYKIRI DEÄERLER
    print("="*60)
    print("5. AYKIRI DEÄERLER: ORTA-YÃœKSEK")
    print("="*60)
    
    print(f"""
   â€¢ Ortalama aykÄ±rÄ± oran: %{outlier_summary['Avg Outlier Rate']:.2f}
   â€¢ >%10 aykÄ±rÄ±lÄ±k gÃ¶steren sÃ¼tun: {outlier_summary['High Outlier Cols (>10%)']}
   
   Ã–NERÄ°: RobustScaler, winsorization, tree-based modeller
""")
    
    # MODEL KURULUM Ã–NERÄ°LERÄ°
    print("\n" + "="*60)
    print("MODEL KURULUM Ã–NERÄ°LERÄ°")
    print("="*60)
    
    print("""
   1. Ã–N Ä°ÅLEME ADIMLARI:
      â€¢ %40+ eksik sÃ¼tunlarÄ± sil
      â€¢ Sabit deÄŸerli sÃ¼tunlarÄ± sil
      â€¢ IterativeImputer ile eksik deÄŸer doldur
      â€¢ RobustScaler ile Ã¶lÃ§ekleme
      â€¢ PCA veya feature selection ile boyut azalt
   
   2. DENGESÄ°ZLÄ°K Ã‡Ã–ZÃœMÃœ:
      â€¢ SMOTE veya class_weight='balanced'
      â€¢ Threshold optimizasyonu
   
   3. MODEL SEÃ‡Ä°MÄ°:
      â€¢ Random Forest / XGBoost / LightGBM
      â€¢ Tree-based modeller aykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±
   
   4. DEÄERLENDÄ°RME:
      â€¢ Stratified K-Fold Cross Validation
      â€¢ F1-Score ve PR-AUC metrikleri
      â€¢ Confusion Matrix analizi
""")
    
    print("\n" + "="*80)
    print("                    ANALÄ°Z TAMAMLANDI")
    print("="*80)


# =============================================================================
# ANA FONKSÄ°YON
# =============================================================================
def run_full_eda(data_path, output_dir='./eda_outputs/'):
    """
    Tam EDA analizini Ã§alÄ±ÅŸtÄ±rÄ±r.
    
    Parameters:
    -----------
    data_path : str
        Veri dosyasÄ±nÄ±n yolu
    output_dir : str
        Ã‡Ä±ktÄ± klasÃ¶rÃ¼
    
    Returns:
    --------
    dict : TÃ¼m analiz sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k
    """
    
    # Ã‡Ä±ktÄ± klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # Veri yÃ¼kleme
    print("Veri yÃ¼kleniyor...")
    df = pd.read_csv(data_path)
    
    target_col = 'Pass/Fail'
    feature_cols = [col for col in df.columns if col not in ['Time', 'Pass/Fail']]
    
    print(f"âœ“ Veri yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
    
    # TÃ¼m bÃ¶lÃ¼mleri Ã§alÄ±ÅŸtÄ±r
    results = {}
    
    # BÃ¶lÃ¼m 1
    results['summary'] = bolum1_veri_tanitimi(df, target_col, feature_cols)
    
    # BÃ¶lÃ¼m 2
    missing_stats, missing_df = bolum2_eksik_veri_analizi(df, target_col, feature_cols, output_dir)
    results['missing'] = missing_stats
    
    # BÃ¶lÃ¼m 3
    stats_df = bolum3_tanimlayici_istatistikler(df, feature_cols, output_dir)
    results['descriptive'] = stats_df
    
    # BÃ¶lÃ¼m 4
    class_stats = bolum4_sinif_dengesizligi(df, target_col, output_dir)
    results['class_balance'] = class_stats
    
    # BÃ¶lÃ¼m 5
    corr_stats, target_corr = bolum5_korelasyon_analizi(df, target_col, feature_cols, output_dir)
    results['correlation'] = corr_stats
    
    # BÃ¶lÃ¼m 6
    outlier_summary, outlier_df = bolum6_aykiri_deger_analizi(df, feature_cols, output_dir)
    results['outliers'] = outlier_summary
    
    # BÃ¶lÃ¼m 7
    sensor_analysis = bolum7_hedef_sensor_iliskisi(df, target_col, feature_cols, target_corr, output_dir)
    results['sensor_analysis'] = sensor_analysis
    
    # BÃ¶lÃ¼m 8
    bolum8_sonuc_ozet(df, feature_cols, target_col, missing_stats, class_stats, 
                      corr_stats, outlier_summary)
    
    print(f"\nâœ“ TÃ¼m gÃ¶rseller '{output_dir}' klasÃ¶rÃ¼ne kaydedildi.")
    
    return results


# =============================================================================
# Ã‡ALIÅTIRMA
# =============================================================================
if __name__ == "__main__":
    
    # Veri dosyasÄ± yolu - KENDÄ° YOLUNUZU YAZIN
    DATA_PATH = 'Downloads/Buket/uci-secom.csv'
    OUTPUT_DIR = './eda_outputs/'
    
    # Tam EDA analizini Ã§alÄ±ÅŸtÄ±r
    results = run_full_eda(DATA_PATH, OUTPUT_DIR)
    
    print("\n" + "="*80)
    print("EDA ANALÄ°ZÄ° BAÅARIYLA TAMAMLANDI!")
    print("="*80)