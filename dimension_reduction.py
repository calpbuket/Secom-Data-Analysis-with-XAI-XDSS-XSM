"""
SECOM Veri Seti - BOYUT AZALTMA ANALÄ°ZÄ° (AÅAMA 3)
==================================================
Mevcut pipeline: IterativeImputer â†’ RobustScaler â†’ SMOTE â†’ XGBoost (SABÄ°T)

Deneyler:
    A) PCA: %90, %95, %99 varyans seviyeleri
    B) Feature Selection: Top-50, Top-100, Top-150 features (XGBoost importance)

Gerekli paketler:
    pip install imbalanced-learn xgboost
"""

import warnings
warnings.filterwarnings('ignore')

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import RobustScaler
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.decomposition import PCA
from sklearn.metrics import (
    f1_score, recall_score, precision_score, roc_auc_score,
    precision_recall_curve, auc, confusion_matrix
)
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE


# =============================================================================
# YARDIMCI FONKSÄ°YONLAR
# =============================================================================

def calculate_missing_ratio(df):
    return df.isnull().sum() / len(df)


def drop_high_missing_columns(X, threshold=0.40):
    missing_ratios = calculate_missing_ratio(X)
    cols_to_drop = missing_ratios[missing_ratios >= threshold].index.tolist()
    X_clean = X.drop(columns=cols_to_drop)
    return X_clean, cols_to_drop


def drop_constant_columns(X):
    constant_cols = []
    for col in X.columns:
        nunique = X[col].dropna().nunique()
        std = X[col].std()
        if nunique <= 1 or (pd.notna(std) and std == 0):
            constant_cols.append(col)
    X_clean = X.drop(columns=constant_cols)
    return X_clean, constant_cols


def pr_auc_score(y_true, y_prob):
    precision, recall, _ = precision_recall_curve(y_true, y_prob, pos_label=1)
    return auc(recall, precision)


def load_and_prepare_data(filepath):
    """Veriyi yÃ¼kle ve hazÄ±rla."""
    print("=" * 70)
    print("VERÄ° HAZIRLAMA")
    print("=" * 70)
    
    df = pd.read_csv(filepath)
    print(f"\n[1] Veri yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
    
    if 'Time' in df.columns:
        df = df.drop(columns=['Time'])
    
    y = df['Pass/Fail']
    X = df.drop(columns=['Pass/Fail'])
    
    X_clean, _ = drop_high_missing_columns(X, threshold=0.40)
    X_clean, _ = drop_constant_columns(X_clean)
    
    y_encoded = (y == 1).astype(int)
    
    print(f"[2] Final boyut: {X_clean.shape[1]} feature")
    print(f"[3] SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±: Pass={sum(y_encoded==0)}, Fail={sum(y_encoded==1)}")
    
    return X_clean, y_encoded


# =============================================================================
# BASELINE SONUÃ‡LARI (KarÅŸÄ±laÅŸtÄ±rma iÃ§in)
# =============================================================================

BASELINE_RESULTS = {
    'F1_fail': 0.1285,
    'Recall_fail': 0.0771,
    'Precision_fail': 0.4705,
    'PR_AUC': 0.1954,
    'ROC_AUC': 0.7054
}


# =============================================================================
# A) PCA DENEYÄ°
# =============================================================================

def run_pca_experiment(X, y, variance_levels=[0.90, 0.95, 0.99]):
    """
    PCA ile boyut azaltma deneyi.
    Pipeline: IterativeImputer â†’ RobustScaler â†’ PCA â†’ SMOTE â†’ XGBoost
    """
    print("\n" + "=" * 70)
    print("A) PCA DENEYÄ°")
    print("Pipeline: Imputer â†’ Scaler â†’ PCA â†’ SMOTE â†’ XGBoost")
    print("=" * 70)
    
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    xgb_params = {
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'scale_pos_weight': 1.0,
        'random_state': 42,
        'n_jobs': -1,
        'eval_metric': 'logloss'
    }
    
    X_array = X.values if hasattr(X, 'values') else X
    y_array = y.values if hasattr(y, 'values') else y
    
    results = []
    
    for var_level in variance_levels:
        print(f"\n--- PCA Varyans: {var_level*100:.0f}% ---")
        start_time = time.time()
        
        fold_scores = {
            'f1_fail': [], 'recall_fail': [], 'precision_fail': [],
            'pr_auc': [], 'roc_auc': [], 'f1_macro': []
        }
        n_components_list = []
        
        for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_array, y_array)):
            X_train, X_test = X_array[train_idx], X_array[test_idx]
            y_train, y_test = y_array[train_idx], y_array[test_idx]
            
            # 1. Imputation
            imputer = IterativeImputer(
                estimator=ExtraTreesRegressor(n_estimators=10, random_state=42, n_jobs=-1),
                max_iter=10, random_state=42
            )
            X_train_imp = imputer.fit_transform(X_train)
            X_test_imp = imputer.transform(X_test)
            
            # 2. Scaling
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train_imp)
            X_test_scaled = scaler.transform(X_test_imp)
            
            # 3. PCA
            pca = PCA(n_components=var_level, random_state=42)
            X_train_pca = pca.fit_transform(X_train_scaled)
            X_test_pca = pca.transform(X_test_scaled)
            n_components_list.append(pca.n_components_)
            
            # 4. SMOTE
            smote = SMOTE(random_state=42)
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_pca, y_train)
            
            # 5. Model
            model = XGBClassifier(**xgb_params)
            model.fit(X_train_resampled, y_train_resampled)
            
            # Tahminler
            y_pred = model.predict(X_test_pca)
            y_prob = model.predict_proba(X_test_pca)[:, 1]
            
            # Metrikler
            fold_scores['f1_fail'].append(f1_score(y_test, y_pred, pos_label=1))
            fold_scores['recall_fail'].append(recall_score(y_test, y_pred, pos_label=1))
            fold_scores['precision_fail'].append(precision_score(y_test, y_pred, pos_label=1, zero_division=0))
            fold_scores['pr_auc'].append(pr_auc_score(y_test, y_prob))
            fold_scores['roc_auc'].append(roc_auc_score(y_test, y_prob))
            fold_scores['f1_macro'].append(f1_score(y_test, y_pred, average='macro'))
            
            print(f"  Fold {fold_idx+1}/5 âœ“ (n_components={pca.n_components_})")
        
        elapsed = time.time() - start_time
        avg_components = int(np.mean(n_components_list))
        
        result = {
            'PCA_Varyans': f"{var_level*100:.0f}%",
            'BileÅŸen_SayÄ±sÄ±': avg_components,
            'F1_fail_mean': np.mean(fold_scores['f1_fail']),
            'F1_fail_std': np.std(fold_scores['f1_fail']),
            'Recall_fail_mean': np.mean(fold_scores['recall_fail']),
            'Recall_fail_std': np.std(fold_scores['recall_fail']),
            'Precision_fail_mean': np.mean(fold_scores['precision_fail']),
            'PR_AUC_mean': np.mean(fold_scores['pr_auc']),
            'PR_AUC_std': np.std(fold_scores['pr_auc']),
            'ROC_AUC_mean': np.mean(fold_scores['roc_auc']),
            'F1_macro_mean': np.mean(fold_scores['f1_macro']),
            'SÃ¼re_s': elapsed
        }
        results.append(result)
        
        print(f"  â†’ F1_fail: {result['F1_fail_mean']:.4f} | Recall: {result['Recall_fail_mean']:.4f} | PR-AUC: {result['PR_AUC_mean']:.4f}")
        print(f"  â†’ SÃ¼re: {elapsed:.1f}s")
    
    return pd.DataFrame(results)


# =============================================================================
# B) FEATURE SELECTION DENEYÄ°
# =============================================================================

def get_feature_importance(X, y):
    """
    TÃ¼m veri Ã¼zerinde XGBoost eÄŸitip feature importance Ã§Ä±kar.
    Bu importance deÄŸerleri feature selection iÃ§in kullanÄ±lacak.
    """
    print("\n[*] Feature importance hesaplanÄ±yor...")
    
    X_array = X.values if hasattr(X, 'values') else X
    y_array = y.values if hasattr(y, 'values') else y
    
    # Imputation
    imputer = IterativeImputer(
        estimator=ExtraTreesRegressor(n_estimators=10, random_state=42, n_jobs=-1),
        max_iter=10, random_state=42
    )
    X_imp = imputer.fit_transform(X_array)
    
    # Scaling
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X_imp)
    
    # SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_scaled, y_array)
    
    # Model
    model = XGBClassifier(
        n_estimators=100, max_depth=6, learning_rate=0.1,
        scale_pos_weight=1.0, random_state=42, n_jobs=-1, eval_metric='logloss'
    )
    model.fit(X_resampled, y_resampled)
    
    # Feature importance
    importance = model.feature_importances_
    
    # SÄ±ralama
    feature_names = X.columns if hasattr(X, 'columns') else [f'f{i}' for i in range(X_array.shape[1])]
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False).reset_index(drop=True)
    
    print(f"[*] Top 10 feature:")
    for i, row in importance_df.head(10).iterrows():
        print(f"    {i+1}. {row['feature']}: {row['importance']:.4f}")
    
    return importance_df


def run_feature_selection_experiment(X, y, importance_df, top_k_values=[50, 100, 150]):
    """
    Feature Selection deneyi.
    Pipeline: IterativeImputer â†’ RobustScaler â†’ Feature Selection â†’ SMOTE â†’ XGBoost
    """
    print("\n" + "=" * 70)
    print("B) FEATURE SELECTION DENEYÄ°")
    print("Pipeline: Imputer â†’ Scaler â†’ Top-K Features â†’ SMOTE â†’ XGBoost")
    print("=" * 70)
    
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    xgb_params = {
        'n_estimators': 100,
        'max_depth': 6,
        'learning_rate': 0.1,
        'scale_pos_weight': 1.0,
        'random_state': 42,
        'n_jobs': -1,
        'eval_metric': 'logloss'
    }
    
    X_array = X.values if hasattr(X, 'values') else X
    y_array = y.values if hasattr(y, 'values') else y
    feature_names = X.columns.tolist() if hasattr(X, 'columns') else list(range(X_array.shape[1]))
    
    results = []
    
    for top_k in top_k_values:
        print(f"\n--- Top-{top_k} Features ---")
        start_time = time.time()
        
        # SeÃ§ilecek feature'larÄ±n indexlerini bul
        top_features = importance_df.head(top_k)['feature'].tolist()
        feature_indices = [feature_names.index(f) for f in top_features if f in feature_names]
        
        if len(feature_indices) < top_k:
            print(f"  [!] UyarÄ±: {top_k} yerine {len(feature_indices)} feature bulundu")
        
        fold_scores = {
            'f1_fail': [], 'recall_fail': [], 'precision_fail': [],
            'pr_auc': [], 'roc_auc': [], 'f1_macro': []
        }
        
        for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_array, y_array)):
            X_train, X_test = X_array[train_idx], X_array[test_idx]
            y_train, y_test = y_array[train_idx], y_array[test_idx]
            
            # 1. Imputation (tÃ¼m feature'lar Ã¼zerinde)
            imputer = IterativeImputer(
                estimator=ExtraTreesRegressor(n_estimators=10, random_state=42, n_jobs=-1),
                max_iter=10, random_state=42
            )
            X_train_imp = imputer.fit_transform(X_train)
            X_test_imp = imputer.transform(X_test)
            
            # 2. Scaling
            scaler = RobustScaler()
            X_train_scaled = scaler.fit_transform(X_train_imp)
            X_test_scaled = scaler.transform(X_test_imp)
            
            # 3. Feature Selection (scaled data Ã¼zerinde)
            X_train_selected = X_train_scaled[:, feature_indices]
            X_test_selected = X_test_scaled[:, feature_indices]
            
            # 4. SMOTE
            smote = SMOTE(random_state=42)
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_selected, y_train)
            
            # 5. Model
            model = XGBClassifier(**xgb_params)
            model.fit(X_train_resampled, y_train_resampled)
            
            # Tahminler
            y_pred = model.predict(X_test_selected)
            y_prob = model.predict_proba(X_test_selected)[:, 1]
            
            # Metrikler
            fold_scores['f1_fail'].append(f1_score(y_test, y_pred, pos_label=1))
            fold_scores['recall_fail'].append(recall_score(y_test, y_pred, pos_label=1))
            fold_scores['precision_fail'].append(precision_score(y_test, y_pred, pos_label=1, zero_division=0))
            fold_scores['pr_auc'].append(pr_auc_score(y_test, y_prob))
            fold_scores['roc_auc'].append(roc_auc_score(y_test, y_prob))
            fold_scores['f1_macro'].append(f1_score(y_test, y_pred, average='macro'))
            
            print(f"  Fold {fold_idx+1}/5 âœ“")
        
        elapsed = time.time() - start_time
        
        result = {
            'Feature_SayÄ±sÄ±': top_k,
            'F1_fail_mean': np.mean(fold_scores['f1_fail']),
            'F1_fail_std': np.std(fold_scores['f1_fail']),
            'Recall_fail_mean': np.mean(fold_scores['recall_fail']),
            'Recall_fail_std': np.std(fold_scores['recall_fail']),
            'Precision_fail_mean': np.mean(fold_scores['precision_fail']),
            'PR_AUC_mean': np.mean(fold_scores['pr_auc']),
            'PR_AUC_std': np.std(fold_scores['pr_auc']),
            'ROC_AUC_mean': np.mean(fold_scores['roc_auc']),
            'F1_macro_mean': np.mean(fold_scores['f1_macro']),
            'SÃ¼re_s': elapsed
        }
        results.append(result)
        
        print(f"  â†’ F1_fail: {result['F1_fail_mean']:.4f} | Recall: {result['Recall_fail_mean']:.4f} | PR-AUC: {result['PR_AUC_mean']:.4f}")
        print(f"  â†’ SÃ¼re: {elapsed:.1f}s")
    
    return pd.DataFrame(results)


# =============================================================================
# SONUÃ‡ RAPORLAMA
# =============================================================================

def generate_comparison_report(pca_results, fs_results):
    """PCA vs Feature Selection karÅŸÄ±laÅŸtÄ±rma raporu."""
    
    print("\n" + "=" * 70)
    print("KARÅILAÅTIRMA RAPORU")
    print("=" * 70)
    
    # Baseline deÄŸerleri
    print("\n--- BASELINE (442 feature, PCA/FS yok) ---")
    print(f"  F1_fail: {BASELINE_RESULTS['F1_fail']:.4f}")
    print(f"  Recall_fail: {BASELINE_RESULTS['Recall_fail']:.4f}")
    print(f"  PR-AUC: {BASELINE_RESULTS['PR_AUC']:.4f}")
    
    # PCA SonuÃ§larÄ±
    print("\n--- A) PCA SONUÃ‡LARI ---\n")
    pca_display = pca_results[['PCA_Varyans', 'BileÅŸen_SayÄ±sÄ±', 'F1_fail_mean', 'Recall_fail_mean', 'PR_AUC_mean', 'SÃ¼re_s']].copy()
    pca_display.columns = ['Varyans', 'BileÅŸen', 'F1_fail', 'Recall', 'PR-AUC', 'SÃ¼re(s)']
    print(pca_display.to_string(index=False))
    
    # En iyi PCA
    best_pca_idx = pca_results['PR_AUC_mean'].idxmax()
    best_pca = pca_results.loc[best_pca_idx]
    print(f"\n  â˜… En iyi PCA: {best_pca['PCA_Varyans']} ({best_pca['BileÅŸen_SayÄ±sÄ±']} bileÅŸen)")
    print(f"    PR-AUC: {best_pca['PR_AUC_mean']:.4f} (Baseline'dan {best_pca['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']:+.4f})")
    
    # Feature Selection SonuÃ§larÄ±
    print("\n--- B) FEATURE SELECTION SONUÃ‡LARI ---\n")
    fs_display = fs_results[['Feature_SayÄ±sÄ±', 'F1_fail_mean', 'Recall_fail_mean', 'PR_AUC_mean', 'SÃ¼re_s']].copy()
    fs_display.columns = ['Features', 'F1_fail', 'Recall', 'PR-AUC', 'SÃ¼re(s)']
    print(fs_display.to_string(index=False))
    
    # En iyi Feature Selection
    best_fs_idx = fs_results['PR_AUC_mean'].idxmax()
    best_fs = fs_results.loc[best_fs_idx]
    print(f"\n  â˜… En iyi Feature Selection: Top-{int(best_fs['Feature_SayÄ±sÄ±'])} features")
    print(f"    PR-AUC: {best_fs['PR_AUC_mean']:.4f} (Baseline'dan {best_fs['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']:+.4f})")
    
    # Genel KarÅŸÄ±laÅŸtÄ±rma
    print("\n" + "-" * 70)
    print("GENEL KARÅILAÅTIRMA")
    print("-" * 70)
    
    comparison_data = [
        {
            'YÃ¶ntem': 'Baseline (442 feat)',
            'Boyut': 442,
            'F1_fail': BASELINE_RESULTS['F1_fail'],
            'Recall': BASELINE_RESULTS['Recall_fail'],
            'PR-AUC': BASELINE_RESULTS['PR_AUC'],
            'Î”PR-AUC': 0.0
        },
        {
            'YÃ¶ntem': f"PCA {best_pca['PCA_Varyans']}",
            'Boyut': best_pca['BileÅŸen_SayÄ±sÄ±'],
            'F1_fail': best_pca['F1_fail_mean'],
            'Recall': best_pca['Recall_fail_mean'],
            'PR-AUC': best_pca['PR_AUC_mean'],
            'Î”PR-AUC': best_pca['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']
        },
        {
            'YÃ¶ntem': f"Top-{int(best_fs['Feature_SayÄ±sÄ±'])} Features",
            'Boyut': int(best_fs['Feature_SayÄ±sÄ±']),
            'F1_fail': best_fs['F1_fail_mean'],
            'Recall': best_fs['Recall_fail_mean'],
            'PR-AUC': best_fs['PR_AUC_mean'],
            'Î”PR-AUC': best_fs['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']
        }
    ]
    
    comparison_df = pd.DataFrame(comparison_data)
    print("\n")
    print(comparison_df.to_string(index=False))
    
    # Kazanan
    print("\n" + "-" * 70)
    
    if best_pca['PR_AUC_mean'] > best_fs['PR_AUC_mean'] and best_pca['PR_AUC_mean'] > BASELINE_RESULTS['PR_AUC']:
        winner = f"PCA {best_pca['PCA_Varyans']}"
        winner_prauc = best_pca['PR_AUC_mean']
    elif best_fs['PR_AUC_mean'] > BASELINE_RESULTS['PR_AUC']:
        winner = f"Feature Selection (Top-{int(best_fs['Feature_SayÄ±sÄ±'])})"
        winner_prauc = best_fs['PR_AUC_mean']
    else:
        winner = "Baseline (boyut azaltma fayda saÄŸlamadÄ±)"
        winner_prauc = BASELINE_RESULTS['PR_AUC']
    
    print(f"ğŸ† KAZANAN: {winner}")
    print(f"   PR-AUC: {winner_prauc:.4f}")
    
    return comparison_df


def generate_thesis_text(pca_results, fs_results, comparison_df):
    """Tez iÃ§in Ã¶zet paragraf."""
    
    print("\n" + "=" * 70)
    print("TEZ Ä°Ã‡Ä°N Ã–ZET PARAGRAF")
    print("=" * 70)
    
    best_pca = pca_results.loc[pca_results['PR_AUC_mean'].idxmax()]
    best_fs = fs_results.loc[fs_results['PR_AUC_mean'].idxmax()]
    
    text = f"""
### 4.X.X Boyut Azaltma Tekniklerinin DeÄŸerlendirilmesi

SECOM veri setinin yÃ¼ksek boyutluluÄŸu (442 feature) nedeniyle, model performansÄ± 
Ã¼zerindeki etkisini deÄŸerlendirmek amacÄ±yla iki farklÄ± boyut azaltma tekniÄŸi 
incelenmiÅŸtir: Principal Component Analysis (PCA) ve XGBoost tabanlÄ± Feature Selection.

**PCA Analizi:**
ÃœÃ§ farklÄ± varyans aÃ§Ä±klama oranÄ± (%90, %95, %99) test edilmiÅŸtir. 
{best_pca['PCA_Varyans']} varyans seviyesi {int(best_pca['BileÅŸen_SayÄ±sÄ±'])} bileÅŸen ile 
en iyi sonucu vermiÅŸ, PR-AUC deÄŸeri {best_pca['PR_AUC_mean']:.4f} olarak Ã¶lÃ§Ã¼lmÃ¼ÅŸtÃ¼r.
Baseline'a gÃ¶re deÄŸiÅŸim: {best_pca['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']:+.4f}

**Feature Selection Analizi:**
XGBoost feature importance skorlarÄ±na gÃ¶re en Ã¶nemli 50, 100 ve 150 feature 
seÃ§ilerek deneyler yapÄ±lmÄ±ÅŸtÄ±r. Top-{int(best_fs['Feature_SayÄ±sÄ±'])} feature ile 
PR-AUC {best_fs['PR_AUC_mean']:.4f} deÄŸerine ulaÅŸÄ±lmÄ±ÅŸtÄ±r.
Baseline'a gÃ¶re deÄŸiÅŸim: {best_fs['PR_AUC_mean'] - BASELINE_RESULTS['PR_AUC']:+.4f}

**SonuÃ§:**
{'Boyut azaltma teknikleri baseline performansÄ±nÄ± iyileÅŸtirmiÅŸtir.' if max(best_pca['PR_AUC_mean'], best_fs['PR_AUC_mean']) > BASELINE_RESULTS['PR_AUC'] else 'Boyut azaltma teknikleri baseline performansÄ±nÄ± iyileÅŸtirmemiÅŸtir.'}
{'PCA' if best_pca['PR_AUC_mean'] > best_fs['PR_AUC_mean'] else 'Feature Selection'} yÃ¶ntemi 
daha yÃ¼ksek PR-AUC deÄŸeri elde etmiÅŸtir. Ancak, dengesiz veri setlerinde kÃ¼Ã§Ã¼k 
performans farklarÄ±nÄ±n istatistiksel anlamlÄ±lÄ±ÄŸÄ± gÃ¶z Ã¶nÃ¼nde bulundurulmalÄ±dÄ±r.

Feature selection yÃ¶nteminin yorumlanabilirlik avantajÄ±, Ã¼retim ortamÄ±nda hangi 
sensÃ¶rlerin kritik olduÄŸunun belirlenmesi aÃ§Ä±sÄ±ndan deÄŸerlidir. Bu nedenle, 
operasyonel uygulamalar iÃ§in feature selection tercih edilebilir.
"""
    
    print(text)
    return text


# =============================================================================
# ANA FONKSÄ°YON
# =============================================================================

def main(filepath='secom.csv'):
    print("\n")
    print("*" * 70)
    print("  SECOM - BOYUT AZALTMA ANALÄ°ZÄ° (AÅAMA 3)")
    print("  Pipeline: IterativeImputer â†’ RobustScaler â†’ [PCA/FS] â†’ SMOTE â†’ XGBoost")
    print("*" * 70)
    
    # 1. Veri hazÄ±rlama
    X_clean, y = load_and_prepare_data(filepath)
    
    # 2. Feature importance hesapla (Feature Selection iÃ§in)
    importance_df = get_feature_importance(X_clean, y)
    importance_df.to_csv('secom_feature_importance.csv', index=False)
    print("[*] Feature importance kaydedildi: secom_feature_importance.csv")
    
    # 3. PCA Deneyi
    pca_results = run_pca_experiment(X_clean, y, variance_levels=[0.90, 0.95, 0.99])
    pca_results.to_csv('secom_pca_results.csv', index=False)
    
    # 4. Feature Selection Deneyi
    fs_results = run_feature_selection_experiment(X_clean, y, importance_df, top_k_values=[50, 100, 150])
    fs_results.to_csv('secom_feature_selection_results.csv', index=False)
    
    # 5. KarÅŸÄ±laÅŸtÄ±rma Raporu
    comparison_df = generate_comparison_report(pca_results, fs_results)
    comparison_df.to_csv('secom_dimension_reduction_comparison.csv', index=False)
    
    # 6. Tez Ã–zeti
    thesis_text = generate_thesis_text(pca_results, fs_results, comparison_df)
    
    # Tez metnini kaydet
    with open('secom_dimension_reduction_thesis.txt', 'w', encoding='utf-8') as f:
        f.write(thesis_text)
    
    print("\n" + "=" * 70)
    print("[âœ“] TÃœM SONUÃ‡LAR KAYDEDÄ°LDÄ°:")
    print("    - secom_feature_importance.csv")
    print("    - secom_pca_results.csv")
    print("    - secom_feature_selection_results.csv")
    print("    - secom_dimension_reduction_comparison.csv")
    print("    - secom_dimension_reduction_thesis.txt")
    print("=" * 70)
    
    return pca_results, fs_results, importance_df


# =============================================================================
# Ã‡ALIÅTIR
# =============================================================================

if __name__ == "__main__":
    filepath = "Downloads/Buket/uci-secom.csv"
    pca_results, fs_results, importance_df = main(filepath)
